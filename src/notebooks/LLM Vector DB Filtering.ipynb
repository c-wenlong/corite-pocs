{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM takes in user data & recommends \"sessions\" to the users -> pass into vector database to fetch real sessions. We first initialise our API Keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will initialise our vector DB client, the first query will take about 20 seconds as there will be authentication, subsequent queries will be much shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections=[CollectionDescription(name='star_charts'), CollectionDescription(name='midjourney')]\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "QDRANT_URL=os.getenv(\"QDRANT_URL\")\n",
    "QDRANT_API_KEY=os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "\turl=QDRANT_URL, \n",
    "\tapi_key=QDRANT_API_KEY,\n",
    ")\n",
    "\n",
    "print(qdrant_client.get_collections())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a method to embed the artist's profile and a fetch method to fetch the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "openai_client = OpenAI()\n",
    "openai_client.api_key=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_embedding(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    embeddings = openai_client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=text,\n",
    "        encoding_format=\"float\"\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "def fetch_recommended_sessions(artist_profile, collection_name=os.getenv(\"COLLECTION_NAME\"), limit=5):\n",
    "    artist_embedding = text_to_embedding(artist_profile)\n",
    "    similar_sessions = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=artist_embedding,\n",
    "        limit=limit\n",
    "    )\n",
    "    return [session.payload[\"text\"] for session in similar_sessions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test the accuracy of these queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = text_to_embedding(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
